import express from 'express';
import { Pinecone } from '@pinecone-database/pinecone';
import { pipeline, layer_norm } from '@xenova/transformers';
import { GoogleGenerativeAI } from "@google/generative-ai";
import dotenv from 'dotenv';
import fs from 'fs';

// Cargar las variables de entorno desde el archivo .env
dotenv.config();

// Inicializa Pinecone y el modelo de embeddings
const pc = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
const app = express();
const port = 4000;

// Middleware para parsear las solicitudes JSON
app.use(express.json());

// Contador de preguntas
let question_count = 0;

// FunciÃ³n para registrar las preguntas y respuestas en un archivo de log
const chatResponses = (question, response) => {
  const logEntry = `Pregunta: ${question}\nRespuesta: ${response}\n\n`;
  fs.appendFileSync('chat_responses.log', logEntry, 'utf8');
};

// FunciÃ³n para realizar la consulta a Pinecone y generar respuesta con Gemini
async function consultarYGenerarRespuesta(query) {
  const indexName = "negocios"; // Cambia al nombre correcto de tu Ã­ndice
  const indices = await pc.listIndexes();

  // Verifica si el Ã­ndice existe
  if (Array.isArray(indices.indexes) && indices.indexes.some(index => index.name === indexName)) {
    // Extrae embeddings para la consulta
    const extractor = await pipeline('feature-extraction', 'nomic-ai/nomic-embed-text-v1.5');
    let embeddings = await extractor([query], { pooling: 'mean' });

    // Ajusta la dimensiÃ³n esperada para el Ã­ndice
    const matryoshka_dim = 384; // AsegÃºrate que esta dimensiÃ³n coincida con tu Ã­ndice en Pinecone
    embeddings = layer_norm(embeddings, [embeddings.dims[1]])
      .slice(null, [0, matryoshka_dim])
      .normalize(2, -1)
      .tolist();

    // Realiza la consulta de similitud en Pinecone
    const index = pc.Index(indexName);
    const resultado = await index.query({
      vector: embeddings[0],
      topK: 8,  // NÃºmero de resultados deseados
      includeMetadata: true
    });

    // Extrae los fragmentos relevantes de Pinecone
    const contextoPinecone = resultado.matches
      .map(match => match.metadata.text)
      .join("\n")
      .slice(0, 10000); // Limita la longitud del contexto a 10000 caracteres

    if (!contextoPinecone) {
      return "Lo siento, no pude encontrar informaciÃ³n relevante para tu consulta en la base de datos.";
    }

    // Estructura de prompt para la pregunta
    const prompt = `
    Eres un agente de servicio de un banco con experiencia en la gestiÃ³n de reclamos.  
    Tu tarea es **clasificar la siguiente consulta** en una de las siguientes categorÃ­as:  
    
    1ï¸âƒ£ **Reclamo Operativo**: Problemas con transacciones, cargos indebidos, reembolsos, bloqueo de cuentas, demoras en procesos.  
    2ï¸âƒ£ **Reclamo TÃ©cnico**: Fallos en la app, error en cajeros, problemas con banca en lÃ­nea, accesos denegados, errores de autenticaciÃ³n.  
    3ï¸âƒ£ **Reclamo AtenciÃ³n al Cliente**: Mala atenciÃ³n en sucursales, trato inadecuado de asesores, tiempos de espera largos, falta de respuesta.  
    4ï¸âƒ£ **Otro**: Solo si la pregunta no tiene suficiente contexto o no se relaciona con el banco.  
    
    ðŸ“Œ **Ejemplo 1:** "No puedo acceder a mi cuenta en la app." â†’ Reclamo TÃ©cnico  
    ðŸ“Œ **Ejemplo 2:** "Me cobraron dos veces el mismo pago." â†’ Reclamo Operativo  
    ðŸ“Œ **Ejemplo 3:** "El asesor del banco me tratÃ³ mal." â†’ Reclamo AtenciÃ³n al Cliente  
    ðŸ“Œ **Ejemplo 4:** "Â¿CuÃ¡ndo es el prÃ³ximo partido de fÃºtbol?" â†’ Otro  
    
    AquÃ­ tienes informaciÃ³n relevante de la base de datos que puede ayudar a clasificar:  
    ðŸ“ **Contexto encontrado:**  
    ${contextoPinecone}  
    
    ðŸ”¹ **Clasifica la siguiente pregunta:**  
    â“ **Pregunta:** ${query}  
    
    âœï¸ **Devuelve solo la categorÃ­a exacta (sin explicaciones adicionales)**.
    `;
    
    // Crear una instancia de GoogleGenerativeAI con la API Key desde las variables de entorno
    const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
    const model = genAI.getGenerativeModel({
      model: "gemini-1.0-pro",
    });

    // Iniciar la sesiÃ³n de chat con el modelo y configuraciÃ³n
    const generationConfig = {
      temperature: 1,            // Controla la aleatoriedad de la respuesta
      topP: 0.95,               // Controla el recorte de la probabilidad acumulada
      topK: 40,                 // Controla cuÃ¡ntos posibles tokens usar para la generaciÃ³n
      maxOutputTokens: 8192,    // NÃºmero mÃ¡ximo de tokens de salida
      responseMimeType: "text/plain", // Tipo MIME de la respuesta
    };

    const chatSession = model.startChat({
      generationConfig,
      history: [], // No historial en este caso
    });

    // Enviar el mensaje al modelo Gemini
    const result = await chatSession.sendMessage(prompt);

    // Extraer solo la respuesta esperada
    const respuesta = result.response.text().trim();

    if (!respuesta) {
      return "Lo siento, no pude generar una respuesta adecuada para tu consulta.";
    }

    // Registra la pregunta y la respuesta en el archivo de registro
    chatResponses(query, respuesta);

    return respuesta; // Devuelve solo la respuesta generada sin la frase adicional
  } else {
    return "Lo siento, no pudimos encontrar el Ã­ndice para realizar la consulta.";
  }
}

// Endpoint para recibir la consulta y responder
app.post('/chat', async (req, res) => {
  const { question } = req.body; // Obtiene la pregunta del cuerpo de la solicitud

  if (!question) {
    return res.status(400).json({ error: 'No se proporcionÃ³ ninguna pregunta.' });
  }

  try {
    question_count += 1; // Incrementar contador de preguntas

    const respuesta = await consultarYGenerarRespuesta(question);
    res.json({
      data: respuesta,
      message: "La peticiÃ³n se procesÃ³ correctamente",
      question_count: question_count, // Incluir el contador de preguntas en la respuesta
    });
  } catch (error) {
    console.error("Error en la consulta:", error);
    res.status(500).json({ error: 'OcurriÃ³ un error al procesar tu solicitud.' });
  }
});

// Inicia el servidor en el puerto 3000
app.listen(port, () => {
  console.log(`Servidor corriendo en http://localhost:${port}`);
});
